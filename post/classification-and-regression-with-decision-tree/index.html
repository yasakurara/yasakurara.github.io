<!DOCTYPE html>
<html lang="ja">
<head>
  
    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-162790219-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-162790219-1');
    </script>
  

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <title>決定木で分類と回帰を行う - yasakura</title>
  
  <meta name="description" content="モチベーション  決定木で分類と回帰をやりたい なぜ決定木で分類と回帰ができるのかを知りたい scikit-learnでサクッとやりたい  scikit-learnのDecisionTreeClassifierで分類する 2つの特徴量 $X_0,X_1$ からなる，以下のようなデータセットを分類したい．
from sklearn.datasets import make_moons import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap # style colors = [&#39;#e74c3c&#39;, &#39;#3498db&#39;, &#39;#1abc9c&#39;, &#39;#9b59b6&#39;, &#39;#f1c40f&#39;] # red, blue, green, purple, yellow cmap = ListedColormap(colors) plt.style.use(&#39;seaborn&#39;) X, y = make_moons(n_samples=100, noise=0.25, random_state=0) plt.figure(figsize=(4,4)) plt.scatter(X[:,0], X[:,1], c=y, cmap=cmap, marker=&#39;.&#39;) plt.ylabel(&#39;X0&#39;) plt.xlabel(&#39;X1&#39;) plt.show() scikit-learnのDecisionTreeClassifierで分類した結果が以下である．
 1.10. Decision Trees Users Guide sklearn.tree.DecisionTreeClassifier  from sklearn.datasets import make_moons from matplotlib.">
  
  <link href=/css/style.css rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@600;900&display=swap" rel="stylesheet">
  
  <link rel="icon" href="https://yasakura.me/img/favicon.ico">
  
  <link rel="alternate" type="application/atom+xml" href="https://yasakura.me/index.xml" title="yasakura">
</head>
<body>
  <header class="header">
    <div class="title"><a href="https://yasakura.me/">yasakura</a></div>
    <nav id="navigation">
      <ul class="menu">
        <li><a href="https://yasakura.me//about/">ABOUT ME</a></li>
      </ul>
    </nav>
  </header>
<article class="post post-view">
  <header class="post-header">
    
    <p class="post-meta">2020.4.1</p>
    
    <h1 class="post-title">決定木で分類と回帰を行う</h1>
    <ul class="post-author">
      <li>
        <a href="https://twitter.com/yasakurara"><svg id="twitter-logo" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 400 400"><defs><style>.cls-1{fill:none;}.cls-2{fill:#1da1f2;}</style></defs><title>Twitter_Logo_Blue</title><rect class="cls-1" width="400" height="400"/><path class="cls-2" d="M153.62,301.59c94.34,0,145.94-78.16,145.94-145.94,0-2.22,0-4.43-.15-6.63A104.36,104.36,0,0,0,325,122.47a102.38,102.38,0,0,1-29.46,8.07,51.47,51.47,0,0,0,22.55-28.37,102.79,102.79,0,0,1-32.57,12.45,51.34,51.34,0,0,0-87.41,46.78A145.62,145.62,0,0,1,92.4,107.81a51.33,51.33,0,0,0,15.88,68.47A50.91,50.91,0,0,1,85,169.86c0,.21,0,.43,0,.65a51.31,51.31,0,0,0,41.15,50.28,51.21,51.21,0,0,1-23.16.88,51.35,51.35,0,0,0,47.92,35.62,102.92,102.92,0,0,1-63.7,22A104.41,104.41,0,0,1,75,278.55a145.21,145.21,0,0,0,78.62,23"/></svg>@yasakurara</a>
      </li>
    </ul>
    
    <div class="share">
    <a href="https://twitter.com/share?url=https%3a%2f%2fyasakura.me%2fpost%2fclassification-and-regression-with-decision-tree%2f&text=%e6%b1%ba%e5%ae%9a%e6%9c%a8%e3%81%a7%e5%88%86%e9%a1%9e%e3%81%a8%e5%9b%9e%e5%b8%b0%e3%82%92%e8%a1%8c%e3%81%86 - yasakura" rel="nofollow" target="_blank" class="tw">Twitter</a>
    <a href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fyasakura.me%2fpost%2fclassification-and-regression-with-decision-tree%2f" class="fb" target="_blank" rel="nofollow">Facebook</a>
    <a href="http://b.hatena.ne.jp/add?mode=confirm&url=https%3a%2f%2fyasakura.me%2fpost%2fclassification-and-regression-with-decision-tree%2f&title=%e6%b1%ba%e5%ae%9a%e6%9c%a8%e3%81%a7%e5%88%86%e9%a1%9e%e3%81%a8%e5%9b%9e%e5%b8%b0%e3%82%92%e8%a1%8c%e3%81%86 - yasakura" class="ht" target="_blank" rel="nofollow">Hatena</a>
</div>
  </header>
  <div class="post-content">
    
<h1>Table of Contents</h1>
<nav id="TableOfContents">
<ul>
<li><a href="#モチベーション">モチベーション</a></li>
<li><a href="#scikit-learnのdecisiontreeclassifierで分類する">scikit-learnのDecisionTreeClassifierで分類する</a></li>
<li><a href="#エントロピー-で-情報のあいまいさ-を表現する">「エントロピー」で「情報のあいまいさ」を表現する</a></li>
<li><a href="#エントロピーの大小を基準にして決定木を構築する">エントロピーの大小を基準にして決定木を構築する</a></li>
<li><a href="#ジニ不純度で決定木を構築する">ジニ不純度で決定木を構築する</a></li>
<li><a href="#scikit-learnのdecisiontreeregressorで回帰する">scikit-learnのDecisionTreeRegressorで回帰する</a></li>
<li><a href="#平均二乗誤差を基準にして回帰木を構築する">平均二乗誤差を基準にして回帰木を構築する</a></li>
<li><a href="#平均絶対誤差を基準にして回帰木を構築する">平均絶対誤差を基準にして回帰木を構築する</a></li>
<li><a href="#回帰木では外挿-extrapolate-ができないことに注意する">回帰木では外挿(extrapolate)ができないことに注意する</a></li>
<li><a href="#appendix-決定木-ノード-を可視化するために必要なもの">Appendix. 決定木（ノード）を可視化するために必要なもの</a></li>
</ul>
</nav>

    

<h1 id="モチベーション">モチベーション</h1>

<ul>
<li>決定木で分類と回帰をやりたい</li>
<li>なぜ決定木で分類と回帰ができるのかを知りたい</li>
<li>scikit-learnでサクッとやりたい</li>
</ul>

<h1 id="scikit-learnのdecisiontreeclassifierで分類する">scikit-learnのDecisionTreeClassifierで分類する</h1>

<p>2つの特徴量 $X_0,X_1$ からなる，以下のようなデータセットを分類したい．</p>

<p><img src="/img/dataset_moon.png" alt="dataset_moon" /></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> sklearn.datasets <span style="color:#f92672">import</span> make_moons
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">from</span> matplotlib.colors <span style="color:#f92672">import</span> ListedColormap

<span style="color:#75715e"># style</span>
colors <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;#e74c3c&#39;</span>, <span style="color:#e6db74">&#39;#3498db&#39;</span>, <span style="color:#e6db74">&#39;#1abc9c&#39;</span>, <span style="color:#e6db74">&#39;#9b59b6&#39;</span>, <span style="color:#e6db74">&#39;#f1c40f&#39;</span>] <span style="color:#75715e"># red, blue, green, purple, yellow</span>
cmap <span style="color:#f92672">=</span> ListedColormap(colors)
plt<span style="color:#f92672">.</span>style<span style="color:#f92672">.</span>use(<span style="color:#e6db74">&#39;seaborn&#39;</span>)

X, y <span style="color:#f92672">=</span> make_moons(n_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, noise<span style="color:#f92672">=</span><span style="color:#ae81ff">0.25</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)

plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">4</span>,<span style="color:#ae81ff">4</span>))
plt<span style="color:#f92672">.</span>scatter(X[:,<span style="color:#ae81ff">0</span>], X[:,<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span>y, cmap<span style="color:#f92672">=</span>cmap, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;.&#39;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;X0&#39;</span>)
plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;X1&#39;</span>)


plt<span style="color:#f92672">.</span>show()</code></pre></div>
<p>scikit-learnのDecisionTreeClassifierで分類した結果が以下である．</p>

<ul>
<li><a href="https://scikit-learn.org/stable/modules/tree.html">1.10. Decision Trees Users Guide</a></li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier">sklearn.tree.DecisionTreeClassifier</a></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> sklearn.datasets <span style="color:#f92672">import</span> make_moons
<span style="color:#f92672">from</span> matplotlib.colors <span style="color:#f92672">import</span> ListedColormap
<span style="color:#f92672">from</span> sklearn.tree <span style="color:#f92672">import</span> DecisionTreeClassifier, export_graphviz
<span style="color:#f92672">import</span> pydotplus
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np

<span style="color:#75715e"># style</span>
colors <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;#e74c3c&#39;</span>, <span style="color:#e6db74">&#39;#3498db&#39;</span>, <span style="color:#e6db74">&#39;#1abc9c&#39;</span>, <span style="color:#e6db74">&#39;#9b59b6&#39;</span>, <span style="color:#e6db74">&#39;#f1c40f&#39;</span>] <span style="color:#75715e"># red, blue, green, purple, yellow</span>
cmap <span style="color:#f92672">=</span> ListedColormap(colors)
plt<span style="color:#f92672">.</span>style<span style="color:#f92672">.</span>use(<span style="color:#e6db74">&#39;seaborn&#39;</span>)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">paint_classified_area</span>(ax, clf, X, t, nx<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, ny<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, margin<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>):
    x_min, x_max <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> margin) <span style="color:#f92672">*</span> X[:,<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>min() <span style="color:#f92672">-</span> margin <span style="color:#f92672">*</span> X[:,<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>max(), (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> margin) <span style="color:#f92672">*</span> X[:,<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>max() <span style="color:#f92672">-</span> margin <span style="color:#f92672">*</span> X[:,<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>min()
    y_min, y_max <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> margin) <span style="color:#f92672">*</span> X[:,<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>min() <span style="color:#f92672">-</span> margin <span style="color:#f92672">*</span> X[:,<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>max(), (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> margin) <span style="color:#f92672">*</span> X[:,<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>max() <span style="color:#f92672">-</span> margin <span style="color:#f92672">*</span> X[:,<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>min()
    xx, yy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>meshgrid(np<span style="color:#f92672">.</span>linspace(x_min, x_max, nx), np<span style="color:#f92672">.</span>linspace(y_min, y_max, ny))
    Z <span style="color:#f92672">=</span> (clf<span style="color:#f92672">.</span>predict(np<span style="color:#f92672">.</span>c_[xx<span style="color:#f92672">.</span>ravel(), yy<span style="color:#f92672">.</span>ravel()]))<span style="color:#f92672">.</span>reshape(xx<span style="color:#f92672">.</span>shape)
    ax<span style="color:#f92672">.</span>contourf(xx, yy, Z, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, colors<span style="color:#f92672">=</span>colors)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">export_tree_graph</span>(graph_name, features, classes):
    dot_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;./dtc.dot&#34;</span>
    export_graphviz(dtc, 
                    out_file<span style="color:#f92672">=</span>dot_name, 
                    impurity <span style="color:#f92672">=</span> False, 
                    filled <span style="color:#f92672">=</span> False,
                    class_names<span style="color:#f92672">=</span>classes,
                    feature_names<span style="color:#f92672">=</span>features)
    graph <span style="color:#f92672">=</span> pydotplus<span style="color:#f92672">.</span>graph_from_dot_file(dot_name)
    graph<span style="color:#f92672">.</span>write_png(graph_name)

fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, figsize <span style="color:#f92672">=</span> (<span style="color:#ae81ff">15</span>, <span style="color:#ae81ff">5</span>))

X, y <span style="color:#f92672">=</span> make_moons(n_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, noise<span style="color:#f92672">=</span><span style="color:#ae81ff">0.25</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
<span style="color:#66d9ef">for</span> max_depth, ax <span style="color:#f92672">in</span> zip([<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">9</span>], ax):
    dtc <span style="color:#f92672">=</span> DecisionTreeClassifier(criterion<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;entropy&#34;</span>,max_depth<span style="color:#f92672">=</span>max_depth, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
    dtc<span style="color:#f92672">.</span>fit(X, y)
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;accuracy:{:.3f}&#34;</span><span style="color:#f92672">.</span>format(dtc<span style="color:#f92672">.</span>score(X, y)))

    paint_classified_area(ax, dtc, X, y)
    
    ax<span style="color:#f92672">.</span>scatter(X[:,<span style="color:#ae81ff">0</span>], X[:,<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span>y, cmap<span style="color:#f92672">=</span>cmap, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;.&#39;</span>)
    ax<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#39;max_depth : {}&#39;</span><span style="color:#f92672">.</span>format(max_depth))
    ax<span style="color:#f92672">.</span>set_xlabel(<span style="color:#e6db74">&#34;X [0]&#34;</span>)
    ax<span style="color:#f92672">.</span>set_ylabel(<span style="color:#e6db74">&#34;X [1]&#34;</span>)
    
    export_tree_graph(
        graph_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;dtc{}.png&#34;</span><span style="color:#f92672">.</span>format(max_depth),
        features <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;X[0]&#34;</span>,<span style="color:#e6db74">&#34;X[1]&#34;</span>],
        classes <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;upper&#34;</span>,<span style="color:#e6db74">&#34;lower&#34;</span>])</code></pre></div>
<p><img src="/img/decision_tree_139.png" alt="decision_tree_139" /></p>

<p><img src="/img/dtc139.svg" alt="dtc139" /></p>

<p>DecisionTreeClassifierのcriterionとしてentropyを指定した．最大深さ9の決定木について，深さ6で葉が純粋になっている．</p>

<p>DecisionTreeClassifierの<code>.score(X, y)</code>にて，データへのフィッティング具合を知ることができる．今回は0.780, 0.880, 1.000であり，最大深さ９の場合が（<strong>訓練データセットに</strong>）一番フィットしている事になった．ただし，<code>.score(X, y)</code>が大きいからと言って，未知データの予測精度が高いわけではないことに注意する．訓練データセットに過剰適合していることが常である．</p>

<p>DecisionTreeClassifierの<code>.feature_importances_</code>にて，分割に貢献している特徴量を知ることができる．最大の深さが1の場合の<code>.feature_importances_</code>は[0. 1.]となり，特徴量X1で分割されたことが反映されている．また，最大深さが9の場合の<code>.feature_importances_</code>は[0.49836396 0.50163604]となり，特徴量X0とX1でまんべんなく分割されていることが反映されている．</p>

<h1 id="エントロピー-で-情報のあいまいさ-を表現する">「エントロピー」で「情報のあいまいさ」を表現する</h1>

<p>「情報のあいまいさ」を定量的に表すことについて考えてみる．</p>

<p>コインを投げて表が出る確率は1/2であり，６面のサイコロを投げて１の目が出る確率は1/6である．感覚としては，コインの表裏を予想するよりもサイコロの目を予想する方が「あいまいさ」が増す． また，サイコロを２回投げる場合を予想するとなると，１回分を予想するよりもあいまいになった気がする．</p>

<p>サイコロを投げる事象は独立であるので，サイコロを２回投げたときに出る目は，1/6と1/6の積をとり，1/36の確率で予想できることになる．確率は低くなったが，あいまいさが増したと感じる．</p>

<p>ここまでの話を一般化すると，xが起きる確率をp(x)とおくと，２つの事象x,yが独立なら，p(x,y)=p(x)p(y)が成り立ち，また，x事象に関するあいまいさをh(x)とすると，２つの独立な事象x,yに関して，h(x,y)=h(x)+h(y)が成り立つような「何か」が欲しいということになる．ということで先人は以下の式を情報のあいまいさとした．</p>

<div class="mathjax">
$$
h(x)=-\log_2 {p(x)}
$$
</div>

<p>これなら，確率が低ければ低いほどあいまいになることを表現できるし，確率の積によってあいまいさの和を表現することもできる．対数の底は何でもよく，２にすることが多いらしい．h(x)の単位はbit．</p>

<p>ここで，８つの文字{a,b,c,d,e,f,g,h}を袋に詰め，袋から１文字を取り出すクソゲーをする．８文字それぞれが1/8の確率で取り出せることが保証されている場合，１文字あたりの情報のあいまいさは3bitなので，８文字ぶんのあいまいさの期待値（平均）をとると，このクソゲーの「あいまいさの平均」は3bitとなる．
一方，{a,b,c,d,e,f,g,h}のそれぞれの確率が <sup>1</sup>&frasl;<sub>2</sub>, <sup>1</sup>&frasl;<sub>4</sub>, <sup>1</sup>&frasl;<sub>8</sub>, <sup>1</sup>&frasl;<sub>16</sub>, <sup>1</sup>&frasl;<sub>64</sub>, <sup>1</sup>&frasl;<sub>64</sub>, <sup>1</sup>&frasl;<sub>64</sub>, <sup>1</sup>&frasl;<sub>64</sub> であるとすると，{a,b,c,d,e,f,g,h}のそれぞれの複雑さは1bit, 2bit, 3bit, 4bit, 6bit, 6bit, 6bit, 6bit となる．このあいまいさの期待値（平均）を計算すると2bitになるので，この場合のクソゲーの「あいまいさの平均」は2bitとなる．つまり，{a}が出やすいという情報を知ることで，あいまいさが減ったということを表している．</p>

<p>このような「あいまいさの平均」を<strong>情報エントロピー</strong>といい，各事象の確率次第で「あいまいさの平均」は変化することがわかる．</p>

<p>事象 $X_1,X_2,&hellip;,X_n$ のそれぞれの確率を $p_1,p_2,&hellip;,p_n$ とすると，全事象 $X$ のエントロピー $H[X]$ は以下で表せる．</p>

<div class="mathjax">
$$
H[X]=-\sum_{n} p_i \log_2 {p_i}
$$
</div>

<p>事象 $X=X_1,X_2$ のそれぞれの確率を $p,p-1$ とした場合（＝ベルヌーイ分布に従う場合）のエントロピー $H[X]$ は次の図のように表せる．事象ごとの確率にばらつきがない場合にエントロピーが最も高くなり，ある事象だけ確率が極端に大きい/小さいときに，エントロピーが低くなることがわかる．表（または裏）のみが極端に出やすいコインはエントロピーが低く，表裏が等確率で出るコインはエントロピーが高い．</p>

<p><img src="/img/entropy.png" alt="entropy" /></p>

<h1 id="エントロピーの大小を基準にして決定木を構築する">エントロピーの大小を基準にして決定木を構築する</h1>

<p>エントロピーを求めるには，予め，各事象（各クラス）の確率（割合）を求めておく必要がある．</p>

<p>ノードmに属するクラスkの割合 $p(m,k)$ を求めるための式は以下である． $(x_i,y_i)$ は（特徴量，クラス），Rmはノード（葉）m内のデータ集合，Nmはノードmに属するすべてのデータ数を表す．</p>

<div class="mathjax">
$$
p(m,k) = \frac{1}{N_m} \sum_{x_i \in R_m} I(y_i = k)
$$
</div>

<p>ノードmに関する情報エントロピーは以下の式で求められる．交差エントロピー誤差関数とも呼ぶ．</p>

<div class="mathjax">
$$
H[X_m]=-　\sum_{k} p(m,k) \log_2 {p(m,k)}
$$
</div>

<p>分割前のデータセットをノード0とすると，ノード0のエントロピー $H[X_0]$ は1.0である．これは，クラスが未知の特徴量(X0,X1)が与えられたとき，この特徴量がクラス0なのかクラス1なのか見分けられない（あいまいさが大きい）ことを意味する．</p>

<p>早速，冒頭で紹介した月形のデータセットを分割していく．まず，特徴量X1に着目し，X1を降順にソートした上で，クラスを見てみる．</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> pandas <span style="color:#f92672">as</span> pd

df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame({<span style="color:#e6db74">&#34;X0&#34;</span>:X[:,<span style="color:#ae81ff">0</span>],<span style="color:#e6db74">&#34;X1&#34;</span>:X[:,<span style="color:#ae81ff">1</span>],<span style="color:#e6db74">&#34;_class&#34;</span>:y})
df<span style="color:#f92672">.</span>sort_values(by<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;X1&#34;</span>, ascending<span style="color:#f92672">=</span>False)<span style="color:#f92672">.</span>head(<span style="color:#ae81ff">5</span>)</code></pre></div>
<p>X1が大きい部分はクラス0のみだが，X1が小さくなる方をみていくと，クラス1のデータが出てくる．</p>

<table>
<thead>
<tr>
<th align="right">X0</th>
<th align="right">X1</th>
<th align="right">_class</th>
</tr>
</thead>

<tbody>
<tr>
<td align="right">-0.796819</td>
<td align="right">0.649555</td>
<td align="right">0</td>
</tr>

<tr>
<td align="right">0.724286</td>
<td align="right">0.610817</td>
<td align="right">0</td>
</tr>

<tr>
<td align="right">-0.652006</td>
<td align="right">0.610241</td>
<td align="right">0</td>
</tr>

<tr>
<td align="right">1.114994</td>
<td align="right">0.589595</td>
<td align="right">0</td>
</tr>

<tr>
<td align="right">2.081867</td>
<td align="right">0.572525</td>
<td align="right">1</td>
</tr>
</tbody>
</table>

<p>とりあえず，分割軸をX1=0.589595とX1=0.572525の中間とする．</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">df<span style="color:#f92672">.</span>sort_values(by<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;X1&#34;</span>, ascending<span style="color:#f92672">=</span>False)<span style="color:#f92672">.</span>query(<span style="color:#e6db74">&#39;X1 &gt; 0.581060&#39;</span>)<span style="color:#f92672">.</span>count()

X0       <span style="color:#ae81ff">28</span>
X1       <span style="color:#ae81ff">28</span>
_class    <span style="color:#ae81ff">28</span></code></pre></div>
<p>これで，28点のクラス0のみを含んだノード1と，22点のクラス0，50点のクラス1を含んだノード2に分割したことになる．</p>

<p>ノード1のエントロピー $H[X_1]$ は，ノード1内にクラス0しか含まれていないことから，0.0である．</p>

<p>ノード2のエントロピー $H[X_2]$ は $- \frac{22}{72} \log \frac{22}{72} - \frac{50}{72} \log \frac{50}{72} = 0.615498$ になる．</p>

<p>で，ノード0から各ノードにどれだけデータが分岐したのかを重み付けすることで，ノード1と2全体でのエントロピーを算出する．</p>

<div class="mathjax">
$$
\frac{28}{100} H[X_1] + \frac{72}{100} H[X_2] = 0.443159
$$
</div>

<p>この値はノード0の $H[X_0] = 1.0$ よりも小さいので，今回の分割によってあいまいさを小さくすることができたと言える．つまり，クラスが未知なデータ(X0,X1)が与えられたとき，X1が0.581060より大きな特徴量「クラス0だ」と予想でき，X1が0.581060より小さな特徴量は「クラス１である可能性が高い」と分類することができる．</p>

<p>元ノードと分岐後のノードのエントロピーの差分を「情報ゲイン」と言うらしい．情報ゲインが最大になるような分割軸を探していく．</p>

<p>試しに，更にX1が小さくなる方を見ていく．例えば，X1=0.18065とX1=0.164730の中間である0.172690らへんを分割軸にしてみる．</p>

<table>
<thead>
<tr>
<th align="right">X0</th>
<th align="right">X1</th>
<th align="right">_class</th>
</tr>
</thead>

<tbody>
<tr>
<td align="right">0.162802</td>
<td align="right">0.217376</td>
<td align="right">1</td>
</tr>

<tr>
<td align="right">-1.238857</td>
<td align="right">0.205974</td>
<td align="right">0</td>
</tr>

<tr>
<td align="right">-1.036595</td>
<td align="right">0.195670</td>
<td align="right">0</td>
</tr>

<tr>
<td align="right">0.803637</td>
<td align="right"><strong>0.180650</strong></td>
<td align="right">0</td>
</tr>

<tr>
<td align="right">2.017774</td>
<td align="right"><strong>0.164730</strong></td>
<td align="right">1</td>
</tr>
</tbody>
</table>

<p>X1 &gt; 0.172690 なクラス0は43点，クラス1は11点あるらしい．</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">df<span style="color:#f92672">.</span>sort_values(by<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;X1&#34;</span>, ascending<span style="color:#f92672">=</span>False)<span style="color:#f92672">.</span>query(<span style="color:#e6db74">&#39;X1 &gt; 0.172690&#39;</span>)[<span style="color:#e6db74">&#39;_class&#39;</span>]<span style="color:#f92672">.</span>value_counts()

<span style="color:#ae81ff">0</span>    <span style="color:#ae81ff">43</span>
<span style="color:#ae81ff">1</span>    <span style="color:#ae81ff">11</span>
Name: _class, dtype: int64</code></pre></div>
<p>X1 = 0.172690 で分割された事によってできた２つの領域（ノード）について，それぞれエントロピーを求める．</p>

<div class="mathjax">
$$
H[X_1] = - \frac{43}{54} \log \frac{43}{54} - \frac{11}{54} \log \frac{11}{54}　= 0.505494 
$$
</div>

<div class="mathjax">
$$
H[X_2] = - \frac{7}{46} \log \frac{7}{46} - \frac{39}{46} \log \frac{39}{46}　= 0.426461
$$
</div>

<p>元のノードから分岐したデータの割合で重み付けをする．</p>

<div class="mathjax">
$$
 \frac{54}{100} H[X_1] + \frac{46}{100} H[X_2] = 0.469137
$$
</div>

<p>今回の情報ゲインは先に求めた情報ゲインよりも小さくなってしまったので，先程の分割の方が優秀であると言える．</p>

<p>X0についても同様に検証（分割テストと言う）していき，エントロピーが小さくなるような分割軸を探していく．</p>

<p>ここまでで，深さ（葉）が１の決定木ができたことになる．</p>

<h1 id="ジニ不純度で決定木を構築する">ジニ不純度で決定木を構築する</h1>

<p>DecisionTreeClassifierのcriterionには，&rdquo;gini&rdquo;が指定できる．</p>

<p>ノードmのジニ不純度 $H[X_m]$ は以下の式で与えられる．</p>

<div class="mathjax">
$$
H[X_m] = \sum_{k} p_{mk} (1-p_{mk})
$$
</div>

<p>$p_{mk}$ は先に述べたように，ノードmに属するクラスkの割合を表す． $(x_i,y_i)$ は（特徴量，クラス）， $R_m$ はノードm内のデータ集合， $N_m$ はノードmに属するすべてのデータ数を表す．</p>

<div class="mathjax">
$$
p_{mk} = \frac{1}{N_m} \sum_{x_i \in R_m} I(y_i = k)
$$
</div>

<p>事象 $X=X_1,X_2$ のそれぞれの確率を $p,p-1$ とした場合（＝事象Xがベルヌーイ分布に従う場合）のジニ不純度とエントロピーをプロットしてみた．（ジニ不純度の最大値とエントロピーの最大値が一致するようにジニ不純度を２倍した）</p>

<p><img src="/img/entropy_gini.png" alt="entropy_gini" /></p>

<p>ジニ不純度とエントロピーを比較すると，エントロピーのグラフはp=0.0と1.0付近が急峻であるため，<strong>ノードが純粋であることを好む</strong>．一方，ジニ不純度のグラフはp=0.0と1.0付近が緩やかであるが，0.5付近は急峻であるため，<strong>ノードが純粋であることは強要しないが，p=0.5付近は白黒つけるぞ</strong>という気持ちがある．</p>

<p><img src="/img/comparison_entropy_gini.png" alt="comparison_entropy_gini" /></p>

<p>scikit-learnのDecisionTreeClassifierのcriterionにginiを設定した結果が以下である．</p>

<p><img src="/img/decision_tree_gini_139.png" alt="decision_tree_gini_139" />
<img src="/img/dtc_gini_139.svg" alt="dtc_gini_139" /></p>

<table>
<thead>
<tr>
<th align="right">.score()</th>
<th align="right">.feature_importances_</th>
</tr>
</thead>

<tbody>
<tr>
<td align="right">0.820</td>
<td align="right">[0. 1.]</td>
</tr>

<tr>
<td align="right">0.920</td>
<td align="right">[0.39375244 0.60624756]</td>
</tr>

<tr>
<td align="right">1.000</td>
<td align="right">[0.47248044 0.52751956]</td>
</tr>
</tbody>
</table>

<p>今回は，最大深さ9の場合にて，過剰適合を確認することができる．訓練データに対して<code>.score()</code>が1.0であるが，図を見てみると，気持ち悪い分割の仕方をしている部分がある．</p>

<h1 id="scikit-learnのdecisiontreeregressorで回帰する">scikit-learnのDecisionTreeRegressorで回帰する</h1>

<p>特徴量$X$とラベル $t$ からなる，以下のようなノイズを含んだ正弦波を回帰したい．</p>

<p><img src="/img/sin_wave_with_noise.png" alt="sin_wave_with_noise" /></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">from</span> matplotlib.colors <span style="color:#f92672">import</span> ListedColormap

colors <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;#e74c3c&#39;</span>, <span style="color:#e6db74">&#39;#3498db&#39;</span>, <span style="color:#e6db74">&#39;#1abc9c&#39;</span>, <span style="color:#e6db74">&#39;#9b59b6&#39;</span>, <span style="color:#e6db74">&#39;#f1c40f&#39;</span>] <span style="color:#75715e"># red, blue, green, purple, yellow</span>
cmap <span style="color:#f92672">=</span> ListedColormap(colors)
plt<span style="color:#f92672">.</span>style<span style="color:#f92672">.</span>use(<span style="color:#e6db74">&#39;seaborn&#39;</span>)

seed <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>RandomState(<span style="color:#ae81ff">1</span>)
X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sort(<span style="color:#ae81ff">5</span> <span style="color:#f92672">*</span> seed<span style="color:#f92672">.</span>rand(<span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">1</span>), axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
t <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sin(X)<span style="color:#f92672">.</span>ravel()
t[::<span style="color:#ae81ff">5</span>] <span style="color:#f92672">+=</span> <span style="color:#ae81ff">0.5</span> <span style="color:#f92672">-</span> seed<span style="color:#f92672">.</span>rand(<span style="color:#ae81ff">20</span>)

plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">5</span>,<span style="color:#ae81ff">5</span>))
plt<span style="color:#f92672">.</span>scatter(X, t, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;.&#39;</span>)

plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;X&#34;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;t&#34;</span>)
plt<span style="color:#f92672">.</span>show()</code></pre></div>
<p>scikit-learnのDecisionTreeRegressorで回帰した結果が以下である．</p>

<ul>
<li><a href="https://scikit-learn.org/stable/modules/tree.html">1.10. Decision Trees Users Guide</a></li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor">sklearn.tree.DecisionTreeRegressor</a></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">from</span> matplotlib.colors <span style="color:#f92672">import</span> ListedColormap

colors <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;#e74c3c&#39;</span>, <span style="color:#e6db74">&#39;#3498db&#39;</span>, <span style="color:#e6db74">&#39;#1abc9c&#39;</span>, <span style="color:#e6db74">&#39;#9b59b6&#39;</span>, <span style="color:#e6db74">&#39;#f1c40f&#39;</span>] <span style="color:#75715e"># red, blue, green, purple, yellow</span>
cmap <span style="color:#f92672">=</span> ListedColormap(colors)
plt<span style="color:#f92672">.</span>style<span style="color:#f92672">.</span>use(<span style="color:#e6db74">&#39;seaborn&#39;</span>)

seed <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>RandomState(<span style="color:#ae81ff">1</span>)
X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sort(<span style="color:#ae81ff">5</span> <span style="color:#f92672">*</span> seed<span style="color:#f92672">.</span>rand(<span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">1</span>), axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
t <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sin(X)<span style="color:#f92672">.</span>ravel()
t[::<span style="color:#ae81ff">5</span>] <span style="color:#f92672">+=</span> <span style="color:#ae81ff">0.5</span> <span style="color:#f92672">-</span> seed<span style="color:#f92672">.</span>rand(<span style="color:#ae81ff">20</span>)

<span style="color:#f92672">from</span> sklearn <span style="color:#f92672">import</span> tree

fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, figsize <span style="color:#f92672">=</span> (<span style="color:#ae81ff">15</span>, <span style="color:#ae81ff">5</span>))

<span style="color:#66d9ef">for</span> max_depth, ax <span style="color:#f92672">in</span> zip([<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">9</span>], ax):

    dtr <span style="color:#f92672">=</span> tree<span style="color:#f92672">.</span>DecisionTreeRegressor(criterion<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;mse&#39;</span>, max_depth<span style="color:#f92672">=</span>max_depth)
    dtr<span style="color:#f92672">.</span>fit(X, t)
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;accuracy:{:.3f}&#34;</span><span style="color:#f92672">.</span>format(dtr<span style="color:#f92672">.</span>score(X, y)))

    N <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000</span>
    _X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((N,<span style="color:#ae81ff">1</span>))
    _X[:,<span style="color:#ae81ff">0</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>uniform(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">5.0</span>, N)
    _y <span style="color:#f92672">=</span> dtr<span style="color:#f92672">.</span>predict(_X)

    ax<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#39;max_depth : {}&#39;</span><span style="color:#f92672">.</span>format(max_depth))
    ax<span style="color:#f92672">.</span>scatter(X, t, cmap<span style="color:#f92672">=</span>cmap, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;.&#39;</span>)
    ax<span style="color:#f92672">.</span>scatter(_X, _y, cmap<span style="color:#f92672">=</span>cmap, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;.&#39;</span>)
    ax<span style="color:#f92672">.</span>set_xlabel(<span style="color:#e6db74">&#34;X&#34;</span>)
    ax<span style="color:#f92672">.</span>set_ylabel(<span style="color:#e6db74">&#34;t&#34;</span>)</code></pre></div>
<p><img src="/img/decision_tree_regression_mse_139.png" alt="decision_tree_regression_mse_139" /></p>

<p>今回はDecisionTreeRegressorのcriterionにmse(Mean Squared Error)を指定している．</p>

<p>緑点がtの予測値であり，最大深さ9の場合においてノイズに過剰適合している部分がある．</p>

<h1 id="平均二乗誤差を基準にして回帰木を構築する">平均二乗誤差を基準にして回帰木を構築する</h1>

<p>木モデルは，分割軸を特徴空間の軸に沿わせているために，本質的には，<strong>準最適</strong>となってしまう．特徴軸に対して45度傾いた境界で分割することはできないことに留意する．</p>

<p>上の<code>max_depth : 1</code>の例では，X=3を閾値として，データ・セットを左右に分割している．</p>

<p>回帰木において，ノードm内の最適な予測値 $y(m)$ は，以下のようなノード内の平均値としている． $x_i$ は特徴量，Rmはノードm内のデータ集合，Nmはノードmに属するすべてのデータ数を表す．</p>

<div class="mathjax">
$$
y(m) = \frac{1}{N_m} \sum_{x_i \in R_m} t_i
$$
</div>

<p>ノードmに関する平均二乗誤差(Mean Squared Error)は以下の式で求められる．ノードm内のデータに関して， $y(m)$ からのばらつきを表現するものである．</p>

<div class="mathjax">
$$
H[X_m] = \frac{1}{N_m} \sum_{x_i \in R_m} (t_i - y(m))^2
$$
</div>

<p>分割によってできた２つのノードに対し， $H[X_m]$ をそれぞれ計算し，分岐したデータ数で重み付けした和が元ノードのそれよりも<strong>小さく</strong>なるような閾値をひたすら探していくことで木を構築する．</p>

<h1 id="平均絶対誤差を基準にして回帰木を構築する">平均絶対誤差を基準にして回帰木を構築する</h1>

<p>ノードmに関する平均絶対誤差(Mean Absolute Error)は以下の式で求められる． $y(m)$ は前述した通りである．</p>

<div class="mathjax">
$$
H[X_m] = \frac{1}{N_m} \sum_{x_i \in R_m} |t_i - y(m)|
$$
</div>

<p>DecisionTreeRegressorのcriterionにmaeを指定した結果が以下である．</p>

<p><img src="/img/decision_tree_regression_mae_139.png" alt="decision_tree_regression_mae_139" /></p>

<p>先程のMSEの場合はノイズに過敏であったが，MAEを用いた場合はノイズに対してロバストになっている．</p>

<p>二乗誤差と絶対誤差を比較すると，以下のように誤差(diff)が大きいほど，squared errorは反応しやすいことがわかる．</p>

<p><img src="/img/comparison_squared_absolute.png" alt="comparison_squared_absolute" /></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt

diff <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2.1</span>,<span style="color:#ae81ff">0.1</span>)
squared <span style="color:#f92672">=</span> diff<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>
absolute <span style="color:#f92672">=</span> abs(diff)

plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">5</span>,<span style="color:#ae81ff">5</span>))
plt<span style="color:#f92672">.</span>plot(diff, squared, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;squared error&#39;</span>)
plt<span style="color:#f92672">.</span>plot(diff, absolute, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;absolute error&#39;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;H[X]&#39;</span>)
plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;diff&#39;</span>)
plt<span style="color:#f92672">.</span>legend()
plt<span style="color:#f92672">.</span>show()</code></pre></div>
<h1 id="回帰木では外挿-extrapolate-ができないことに注意する">回帰木では外挿(extrapolate)ができないことに注意する</h1>

<p>今回の例では，0.0 &lt; X &lt; 5.0 な教師データを用いてDecisionTreeRegressorに学習させたが，このXを超えるようなデータを予測させると以下のようになる．</p>

<p><img src="/img/cant_extrapolate.png" alt="cant_extrapolate" /></p>

<h1 id="appendix-決定木-ノード-を可視化するために必要なもの">Appendix. 決定木（ノード）を可視化するために必要なもの</h1>

<p><code>pip install pydotplus</code><br />
<code>pip install graphviz</code><br />
<code>brew install graphviz</code></p>

  </div>
  <footer class="post-footer">
    
    <div class="share">
    <a href="https://twitter.com/share?url=https%3a%2f%2fyasakura.me%2fpost%2fclassification-and-regression-with-decision-tree%2f&text=%e6%b1%ba%e5%ae%9a%e6%9c%a8%e3%81%a7%e5%88%86%e9%a1%9e%e3%81%a8%e5%9b%9e%e5%b8%b0%e3%82%92%e8%a1%8c%e3%81%86 - yasakura" rel="nofollow" target="_blank" class="tw">Twitter</a>
    <a href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fyasakura.me%2fpost%2fclassification-and-regression-with-decision-tree%2f" class="fb" target="_blank" rel="nofollow">Facebook</a>
    <a href="http://b.hatena.ne.jp/add?mode=confirm&url=https%3a%2f%2fyasakura.me%2fpost%2fclassification-and-regression-with-decision-tree%2f&title=%e6%b1%ba%e5%ae%9a%e6%9c%a8%e3%81%a7%e5%88%86%e9%a1%9e%e3%81%a8%e5%9b%9e%e5%b8%b0%e3%82%92%e8%a1%8c%e3%81%86 - yasakura" class="ht" target="_blank" rel="nofollow">Hatena</a>
</div>
  </footer>
</article>


<div class="post-widget">
  <div class="twitter-widget">
    <a class="twitter-timeline" data-lang="en" data-width="300" href="https://twitter.com/yasakurara?ref_src=twsrc%5Etfw">Tweets by yasakurara</a>
    <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
  </div>
</div>


<footer class="footer">
  <nav>
    <ul>
      <li><a href="https://yasakura.me//terms/">Terms of Use</a></li>
    </ul>
  </nav>
  <span>&copy; 2021 yasakura</span>
</footer>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    },
    linebreaks: {
      automatic: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
</body>
</html>

