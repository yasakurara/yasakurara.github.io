<!DOCTYPE html>
<html lang="ja">
<head>
  
    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-162790219-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-162790219-1');
    </script>
  

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <title>劣微分とLassoによるスパース性 - yasakura</title>
  
  <meta name="description" content="モチベーション  部分的に微分できない関数をなんとか微分したい L1正則化(Lasso)を用いると，過学習を防げる場合がある  関数の種類 目的関数の最小値を求めたい機会は多々ある，目的関数を微分することで最小値を求めることがあるが，関数の形によっては，部分的に微分できない場合がある．例えば， $y=\|x\|$ は $x=0$ で微分不可能である．
 関数が連続でない：境界（ $y=\|x\|$ の $x=0$ 付近）において，左極限と右極限が一致しないもの $C_1$ 級関数：1階微分可能であり，それが連続であるもの $C_n$ 級関数：n階微分可能であり，n階の導関数が連続であるもの 滑らかな関数：任意階微分可能であり，任意階の導関数が連続であるもの．$C_\infty$ 級関数．  劣微分・劣勾配 凸関数を最小化するために勾配情報が欲しいが，凸関数はいつでも微分可能であるとは限らない．例えば， $y=|x|$ は凸関数だが， $x=0$ で微分可能でない．このような関数に対して勾配（劣勾配）を定義してみる．
真凸関数 $f:\mathbb{R}^n \to \mathbb{R} \cup \{\infty\}$ かつ $x \in dom(f)$ の元で， $\forall y \in \mathbb{R}^n$ が
$$ f(\boldsymbol{y}) \geq f(\boldsymbol{x}) &#43; \boldsymbol{g}^{\mathrm{T}}(\boldsymbol{y}-\boldsymbol{x}) $$  を満たすとき， $\boldsymbol{g}$ を $\boldsymbol{x}$ における劣勾配という．劣勾配の集合 $\partial f(\boldsymbol{x})$ を劣微分という．
$f(x)=|x|$ の $x=0$ における劣微分は，以下より， $\partial f(0) = [-1,1]$ と求まる．">
  
  <link href=/css/style.css rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@600;900&display=swap" rel="stylesheet">
  
  <link rel="icon" href="https://yasakura.me/img/favicon.ico">
  
  <link rel="alternate" type="application/atom+xml" href="https://yasakura.me/index.xml" title="yasakura">
</head>
<body>
  <header class="header">
    <div class="title"><a href="https://yasakura.me/">yasakura</a></div>
    <nav id="navigation">
      <ul class="menu">
        <li><a href="https://yasakura.me//about/">ABOUT ME</a></li>
      </ul>
    </nav>
  </header>
<article class="post post-view">
  <header class="post-header">
    
    <p class="post-meta">2020.4.1</p>
    
    <h1 class="post-title">劣微分とLassoによるスパース性</h1>
    <ul class="post-author">
      <li>
        <a href="https://twitter.com/yasakurara"><svg id="twitter-logo" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 400 400"><defs><style>.cls-1{fill:none;}.cls-2{fill:#1da1f2;}</style></defs><title>Twitter_Logo_Blue</title><rect class="cls-1" width="400" height="400"/><path class="cls-2" d="M153.62,301.59c94.34,0,145.94-78.16,145.94-145.94,0-2.22,0-4.43-.15-6.63A104.36,104.36,0,0,0,325,122.47a102.38,102.38,0,0,1-29.46,8.07,51.47,51.47,0,0,0,22.55-28.37,102.79,102.79,0,0,1-32.57,12.45,51.34,51.34,0,0,0-87.41,46.78A145.62,145.62,0,0,1,92.4,107.81a51.33,51.33,0,0,0,15.88,68.47A50.91,50.91,0,0,1,85,169.86c0,.21,0,.43,0,.65a51.31,51.31,0,0,0,41.15,50.28,51.21,51.21,0,0,1-23.16.88,51.35,51.35,0,0,0,47.92,35.62,102.92,102.92,0,0,1-63.7,22A104.41,104.41,0,0,1,75,278.55a145.21,145.21,0,0,0,78.62,23"/></svg>@yasakurara</a>
      </li>
    </ul>
    
    <div class="share">
    <a href="https://twitter.com/share?url=https%3a%2f%2fyasakura.me%2fpost%2fsubdifferential%2f&text=%e5%8a%a3%e5%be%ae%e5%88%86%e3%81%a8Lasso%e3%81%ab%e3%82%88%e3%82%8b%e3%82%b9%e3%83%91%e3%83%bc%e3%82%b9%e6%80%a7 - yasakura" rel="nofollow" target="_blank" class="tw">Twitter</a>
    <a href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fyasakura.me%2fpost%2fsubdifferential%2f" class="fb" target="_blank" rel="nofollow">Facebook</a>
    <a href="http://b.hatena.ne.jp/add?mode=confirm&url=https%3a%2f%2fyasakura.me%2fpost%2fsubdifferential%2f&title=%e5%8a%a3%e5%be%ae%e5%88%86%e3%81%a8Lasso%e3%81%ab%e3%82%88%e3%82%8b%e3%82%b9%e3%83%91%e3%83%bc%e3%82%b9%e6%80%a7 - yasakura" class="ht" target="_blank" rel="nofollow">Hatena</a>
</div>
  </header>
  <div class="post-content">
    
<h1>Table of Contents</h1>
<nav id="TableOfContents">
<ul>
<li><a href="#モチベーション">モチベーション</a></li>
<li><a href="#関数の種類">関数の種類</a></li>
<li><a href="#劣微分-劣勾配">劣微分・劣勾配</a></li>
<li><a href="#もっと複雑な関数を微分したい">もっと複雑な関数を微分したい</a></li>
<li><a href="#座標降下法-coodinate-descent-によるl1正則化-lasso-の最適化">座標降下法(Coodinate Descent)によるL1正則化(Lasso)の最適化</a></li>
<li><a href="#lassoを考慮したモデルがなぜスパースになるのかを知るには-ラグランジュの未定乗数法を知ると良い">Lassoを考慮したモデルがなぜスパースになるのかを知るには，ラグランジュの未定乗数法を知ると良い</a>
<ul>
<li><a href="#ラグランジュの未定乗数法とは">ラグランジュの未定乗数法とは</a></li>
<li><a href="#なぜこれで停留点を求めることができるか">なぜこれで停留点を求めることができるか</a></li>
<li><a href="#nabla-g-に対する1次独立性が必要">$\nabla g$ に対する1次独立性が必要</a></li>
<li><a href="#最適性の２次の十分条件を満たせば局所最適解">最適性の２次の十分条件を満たせば局所最適解</a></li>
<li><a href="#ラグランジュ乗数法のkkt条件">ラグランジュ乗数法のKKT条件</a></li>
</ul></li>
<li><a href="#正則化とラグランジュ乗数法">正則化とラグランジュ乗数法</a></li>
<li><a href="#近接勾配法">近接勾配法</a></li>
</ul>
</nav>

    

<h1 id="モチベーション">モチベーション</h1>

<ul>
<li>部分的に微分できない関数をなんとか微分したい</li>
<li>L1正則化(Lasso)を用いると，過学習を防げる場合がある</li>
</ul>

<h1 id="関数の種類">関数の種類</h1>

<p>目的関数の最小値を求めたい機会は多々ある，目的関数を微分することで最小値を求めることがあるが，関数の形によっては，部分的に微分できない場合がある．例えば， $y=\|x\|$ は $x=0$ で微分不可能である．</p>

<ul>
<li>関数が連続でない：境界（ $y=\|x\|$ の $x=0$ 付近）において，左極限と右極限が一致しないもの</li>
<li>$C_1$ 級関数：1階微分可能であり，それが連続であるもの</li>
<li>$C_n$ 級関数：n階微分可能であり，n階の導関数が連続であるもの</li>
<li>滑らかな関数：任意階微分可能であり，任意階の導関数が連続であるもの．$C_\infty$ 級関数．</li>
</ul>

<h1 id="劣微分-劣勾配">劣微分・劣勾配</h1>

<p>凸関数を最小化するために勾配情報が欲しいが，凸関数はいつでも微分可能であるとは限らない．例えば， $y=|x|$ は凸関数だが， $x=0$ で微分可能でない．このような関数に対して勾配（劣勾配）を定義してみる．</p>

<p>真凸関数 $f:\mathbb{R}^n \to \mathbb{R} \cup \{\infty\}$ かつ $x \in dom(f)$ の元で， $\forall y \in \mathbb{R}^n$ が</p>

<div class="mathjax">
$$
f(\boldsymbol{y}) \geq f(\boldsymbol{x}) + \boldsymbol{g}^{\mathrm{T}}(\boldsymbol{y}-\boldsymbol{x})
$$
</div>

<p>を満たすとき， $\boldsymbol{g}$ を $\boldsymbol{x}$ における劣勾配という．劣勾配の集合 $\partial f(\boldsymbol{x})$ を劣微分という．</p>

<p>$f(x)=|x|$ の $x=0$ における劣微分は，以下より， $\partial f(0) = [-1,1]$ と求まる．</p>

<ul>
<li>$y \gt 0$ の場合， $f(y)=|y|=y \geq f(0) + g (y - 0)$ なので，$g \leq 1$</li>
<li>$y = 0$ の場合， $f(y)=|y|=0 \geq f(0) + g (0 - 0)$ なので，$g \in \mathbb{R}$</li>
<li>$y \lt 0$ の場合， $f(y)=|y|=-y \geq f(0) + g (y - 0)$ なので，$g \geq -1$</li>
</ul>

<h1 id="もっと複雑な関数を微分したい">もっと複雑な関数を微分したい</h1>

<p>座標降下法(Coordinate Descent)または勾配降下法(Gradient Descent)を用いる．</p>

<ul>
<li>座標降下法：目的関数の変数を一つずつ最小化する</li>
<li>勾配降下法：あとで書く</li>
</ul>

<h1 id="座標降下法-coodinate-descent-によるl1正則化-lasso-の最適化">座標降下法(Coodinate Descent)によるL1正則化(Lasso)の最適化</h1>

<p>LassoとはLeast absolute shrinkage and selection operatorの略であり，以下の式で表される．以下を最小にするような $\boldsymbol{w}$ を探したいが，微分できない項を含んでいる．ここで， $\boldsymbol{x_n}^{\mathrm{T}} = (1, x_{n1} , x_{n2}, &hellip;, x_{nD})$ ， $\boldsymbol{w}^{\mathrm{T}} = (w_0, w_1, &hellip;, w_D)$ である． $\boldsymbol{w}_{-j}$ は列ベクトル $\boldsymbol{w}$ からj列目を除いたものを表す．</p>

<div class="mathjax">
$$
E(\boldsymbol{w})
=
\frac{1}{2} \sum_{n=1}^{N} \{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x_n} - t_n\}^2 + \lambda \| \boldsymbol{w}_{-0} \|
=
\frac{1}{2} (\boldsymbol{Xw-t}) ^{\mathrm{T}} (\boldsymbol{Xw-t})  + \lambda \| \boldsymbol{w}_{-0} \|
$$
</div>

<p>$w_0$ は正則化項に含まれないので， $E(\boldsymbol{w})$ を $w_0$ で微分することで $w_0$ の最小値が求まる．</p>

<div class="mathjax">
$$
\frac{ \partial E(\boldsymbol{w}) }{ \partial w_0 } = \sum_{n=1}^{N} \{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x_n} - t_n\}

= \sum_{n=1}^{N} \{w_0 + \boldsymbol{w_{-0}}^{\mathrm{T}} \boldsymbol{{x_n}_{-0}} - t_n\} = 0 \\

\therefore w_0
=
\frac{1}{N} \sum_{n=1}^{N} \{t_n - \boldsymbol{w_{-0}}^{\mathrm{T}} \boldsymbol{{x_n}_{-0}}\}
$$
</div>

<p>次に， $E(\boldsymbol{w})$ を $w_k ( \neq 0, 0 \lt k \leq D )$ で偏微分した値を0とする．</p>

<div class="mathjax">
$$
\frac{ \partial E(\boldsymbol{w}) }{ \partial w_k }  = \boldsymbol{X_{(:,k)}^{\mathrm{T}}} \{ \boldsymbol{X_{(:,k)}} w_k + \boldsymbol{X_{(:,-k)}} \boldsymbol{w_{-k}} - \boldsymbol{t} \} + \lambda  sign(w_k) = 0
$$
</div>

<p>$\boldsymbol{X_{(:,k)}}$ は行列Xのk列ベクトルを表し， $\boldsymbol{X_{(:,k)}^{\mathrm{T}}}$ はそれの転置ベクトルを表す．また， $\boldsymbol{X_{(:,-k)}}$ は行列Xからk列ベクトルを除いたものを表す．</p>

<p>$w_k$ について整理すると， $w_k &gt; 0$ の場合と $w_k &lt;0$ の場合ができる．</p>

<div class="mathjax">
$$
\begin{eqnarray}
w_k 
=
\begin{cases}
\frac{1}{\boldsymbol{X_{(:,k)}^{\mathrm{T}}} \boldsymbol{X_{(:,k)}}} \{ \boldsymbol{X_{(:,k)}^{\mathrm{T}}} \{ \boldsymbol{t} - \boldsymbol{X_{(:,-k)}} \boldsymbol{w_{-k}} \} - \lambda\} & ( w_k \gt 0) & (1)\\
\frac{1}{\boldsymbol{X_{(:,k)}^{\mathrm{T}}} \boldsymbol{X_{(:,k)}}} \{ \boldsymbol{X_{(:,k)}^{\mathrm{T}}} \{ \boldsymbol{t} - \boldsymbol{X_{(:,-k)}} \boldsymbol{w_{-k}} \} + \lambda\} & ( w_k \lt 0) & (2)
\end{cases}
\end{eqnarray}
$$
</div>

<p>ここで劣微分を導入する．具体的には，(1)式または(2)式を満たさない場合は問答無用で $w_k=0$ とするだけ．λの大きさによって $w_k=0$ とする範囲を制御できる．これらをまとめてsoft thresholding operatorと呼ぶ．</p>

<p>で，以下の手順に沿って $\boldsymbol{w}$ を求める．以下のように，目的変数（最小化したい変数）を一つずつ触って，目的関数（今回はLasso）を最小化する方法を，座標降下法(Coodinate Descent)という．</p>

<ol>
<li>$\boldsymbol{w}$ を適当な値で初期化</li>
<li>$w_0$ を計算</li>
<li>while $\boldsymbol{w}$ が収束条件を満たすまで

<ul>
<li>for k in 1,2,&hellip;,D

<ul>
<li>$w_k$ を計算</li>
</ul></li>
<li>$w_0$ を計算</li>
</ul></li>
</ol>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> copy

<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MyLasso</span>:
  <span style="color:#66d9ef">def</span> __init__(self, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>, max_iter<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>):
    self<span style="color:#f92672">.</span>alpha <span style="color:#f92672">=</span> alpha
    self<span style="color:#f92672">.</span>max_iter <span style="color:#f92672">=</span> max_iter
    self<span style="color:#f92672">.</span>coef_ <span style="color:#f92672">=</span> None
    self<span style="color:#f92672">.</span>intercept_ <span style="color:#f92672">=</span> None

  <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">fit</span>(self, X, t):
    X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>column_stack((np<span style="color:#f92672">.</span>ones(len(X)),X)) <span style="color:#75715e"># (1,x1,x2,...)</span>
    w <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>])
    
    w[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum(t <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>dot(X[:,<span style="color:#ae81ff">1</span>:], w[<span style="color:#ae81ff">1</span>:]))<span style="color:#f92672">/</span>(X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>])
    
    <span style="color:#66d9ef">for</span> iter <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>max_iter):
        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, len(w)):
            _w <span style="color:#f92672">=</span> copy<span style="color:#f92672">.</span>deepcopy(w)
            _w[i] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>

            w[i] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
            a <span style="color:#f92672">=</span> (np<span style="color:#f92672">.</span>dot(X[:, i], t <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>dot(X, _w)) <span style="color:#f92672">-</span> self<span style="color:#f92672">.</span>alpha)<span style="color:#f92672">/</span>(X[:, i]<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>sum()
            <span style="color:#66d9ef">if</span> a <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0.0</span> : w[i] <span style="color:#f92672">=</span> a
            a <span style="color:#f92672">=</span> (np<span style="color:#f92672">.</span>dot(X[:, i], t <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>dot(X, _w)) <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>alpha)<span style="color:#f92672">/</span>(X[:, i]<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>sum()
            <span style="color:#66d9ef">if</span> a <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">0.0</span> : w[i] <span style="color:#f92672">=</span> a

        w[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum(t <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>dot(X[:,<span style="color:#ae81ff">1</span>:], w[<span style="color:#ae81ff">1</span>:]))<span style="color:#f92672">/</span>(X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>])

        self<span style="color:#f92672">.</span>intercept_ <span style="color:#f92672">=</span> w[<span style="color:#ae81ff">0</span>]
        self<span style="color:#f92672">.</span>coef_ <span style="color:#f92672">=</span> w[<span style="color:#ae81ff">1</span>:]

    <span style="color:#66d9ef">return</span> self

  <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict</span>(self, X):
    y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(X, self<span style="color:#f92672">.</span>coef_)
    y <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>intercept_<span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>ones(len(y))
    <span style="color:#66d9ef">return</span> y</code></pre></div>
<p>$y=x^2+2x+3$ にσ=1.0のノイズを加えた50点を教師データとして，Lassoで予測した結果が以下である．</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split
<span style="color:#f92672">from</span> sklearn.pipeline <span style="color:#f92672">import</span> Pipeline
<span style="color:#f92672">from</span> sklearn.preprocessing <span style="color:#f92672">import</span> PolynomialFeatures

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">f</span>(x):
    <span style="color:#66d9ef">return</span>  x<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span> <span style="color:#f92672">+</span> <span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>x <span style="color:#f92672">+</span> <span style="color:#ae81ff">3</span>

N <span style="color:#f92672">=</span> <span style="color:#ae81ff">50</span>
X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((N,<span style="color:#ae81ff">1</span>))
np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>seed(seed<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
X[:,<span style="color:#ae81ff">0</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>uniform(<span style="color:#f92672">-</span><span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>, N)
noise <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span><span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(N)
t <span style="color:#f92672">=</span> f(X[:,<span style="color:#ae81ff">0</span>])<span style="color:#f92672">+</span> noise

X_train, X_test, t_train, t_test <span style="color:#f92672">=</span> train_test_split(X, t, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)

fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, figsize <span style="color:#f92672">=</span> (<span style="color:#ae81ff">15</span>, <span style="color:#ae81ff">5</span>))

<span style="color:#66d9ef">for</span> degree, ax <span style="color:#f92672">in</span> zip([<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">9</span>],ax):
    pipeline <span style="color:#f92672">=</span> Pipeline([
        (<span style="color:#e6db74">&#39;polynominal_features&#39;</span>, PolynomialFeatures(degree<span style="color:#f92672">=</span>degree)),
        (<span style="color:#e6db74">&#39;linear_regression&#39;</span>, MyLasso(alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>))
    ])

    lr <span style="color:#f92672">=</span> pipeline<span style="color:#f92672">.</span>fit(X_train, t_train)
    <span style="color:#66d9ef">print</span>(pipeline<span style="color:#f92672">.</span>steps[<span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>intercept_)
    <span style="color:#66d9ef">print</span>(pipeline<span style="color:#f92672">.</span>steps[<span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>coef_)
    
    Xcont <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linspace(np<span style="color:#f92672">.</span>min(X), np<span style="color:#f92672">.</span>max(X), <span style="color:#ae81ff">200</span>)<span style="color:#f92672">.</span>reshape((<span style="color:#ae81ff">200</span>, <span style="color:#ae81ff">1</span>))
    ax<span style="color:#f92672">.</span>plot(Xcont, f(Xcont), label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;original&#39;</span>)
    ax<span style="color:#f92672">.</span>plot(X, t,<span style="color:#e6db74">&#39;.&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;training data&#39;</span>)
    ax<span style="color:#f92672">.</span>plot(Xcont, lr<span style="color:#f92672">.</span>predict(Xcont), label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;predictive&#39;</span>)
    ax<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#39;degree : {}&#39;</span><span style="color:#f92672">.</span>format(degree))
    ax<span style="color:#f92672">.</span>legend()
    ax<span style="color:#f92672">.</span>set_xlabel(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;$x$&#39;</span>)
    ax<span style="color:#f92672">.</span>set_ylabel(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;$t$&#39;</span>)</code></pre></div>
<p>多項式モデルで予測し，具体的には１次，３次，５次のモデルで予測してみた．</p>

<p><img src="/img/linear_regression_with_lasso.png" alt="linear_regression_with_lasso" /></p>

<p>各モデルが予測した式は以下（係数を小数点以下第一位に丸めている）であり，３次以上の項の係数は0か非常に小さな値になっている．<strong>これはL1正則化によりスパースなモデルを求めることができたことを意味する．</strong></p>

<p>１次式モデル： $1.8x+5.5$</p>

<p>３次式モデル： $0.0x^3 + 0.9x^2 + 2.1x + 3.1$</p>

<p>５次式モデル： $0.0x^5 + 0.0x^4 - 0.1x^3 + 0.9x^2 + 2.2x + 3.1$</p>

<h1 id="lassoを考慮したモデルがなぜスパースになるのかを知るには-ラグランジュの未定乗数法を知ると良い">Lassoを考慮したモデルがなぜスパースになるのかを知るには，ラグランジュの未定乗数法を知ると良い</h1>

<h2 id="ラグランジュの未定乗数法とは">ラグランジュの未定乗数法とは</h2>

<p>制約条件 $g(x,y)=0$ が与えられた状態で，関数 $f(x,y)$ の最小値または最大値を求めるために便利なのがラグランジュの未定乗数法である．具体的には，ラグランジュ関数 $L(x,y)$ とラグランジュ乗数 $\lambda$ を用いて，以下を解けば良い．</p>

<div class="mathjax">
$$
L(x,y,\lambda) = f(x,y) - \lambda g(x,y) \\
\frac{ \partial L }{ \partial x } = \frac{ \partial L }{ \partial y } = \frac{ \partial L }{ \partial \lambda } = 0
$$
</div>

<h2 id="なぜこれで停留点を求めることができるか">なぜこれで停留点を求めることができるか</h2>

<div class="mathjax">
$$
\frac{ \partial L }{ \partial x } = \frac{ \partial L }{ \partial y } = 0
$$
</div>

<p>を行列で書き直すと，</p>

<div class="mathjax">
$$
\begin{pmatrix} 
\frac{ \partial f(x,y) }{ \partial x } \\ 
\frac{ \partial f(x,y) }{ \partial y }
\end{pmatrix}
=
\lambda
\begin{pmatrix} 
\frac{ \partial g(x,y) }{ \partial x } \\ 
\frac{ \partial g(x,y) }{ \partial y }
\end{pmatrix}
$$
</div>

<p>となり，ベクトル $\partial g$ をλ倍したものがベクトル $\partial f$ であることを表している．では，ベクトル $\partial g$ とは何か．</p>

<p>制約面 $g(x,y)=0$ 上の点 $(x_0,y_0)$ と $(x_0+\delta x, y_0+\delta y)$ を考えると， $(x_0,y_0)$ の周りでのテイラー展開より以下が得られる．</p>

<div class="mathjax">
$$
g(x_0+\delta x, y_0+\delta y) \simeq g(x_0,y_0) + \left. \frac{ \partial g(x,y) }{ \partial x } \right|_{ (x_0,y_0) } \delta x + \left. \frac{ \partial g(x,y) }{ \partial y }\right|_{ (x_0,y_0) } \delta y
$$
</div>

<p>いま， $(x_0,y_0)$ と $(x_0+\delta x, y_0+\delta y)$ は制約面上に存在しているので， $g(x_0,y_0)=g(x_0+\delta x, y_0+\delta y)$ が成立するためには，以下が成り立つ必要がある．</p>

<div class="mathjax">
$$
\left. \frac{ \partial g(x,y) }{ \partial x } \right|_{ (x_0,y_0) } \delta x + \left. \frac{ \partial g(x,y) }{ \partial y } \right|_{ (x_0,y_0) } \delta y = 0
$$
</div>

<p>内積で書き直すと，</p>

<div class="mathjax">
$$
\begin{pmatrix} 
\frac{ \partial g(x,y) }{ \partial x } \\ 
\frac{ \partial g(x,y) }{ \partial y }
\end{pmatrix}_{ (x_0,y_0) }
\cdot
\begin{pmatrix} 
\delta x \\ 
\delta y
\end{pmatrix}

= 0
$$
</div>

<p>$\delta x$ と $\delta y$ が十分に小さければ， $(\delta x, \delta y)$ は点 $(x_0,y_0)$ における制約面 $g$ の接線を表すので，ベクトル $(\frac{ \partial g(x,y) }{ \partial x }, \frac{ \partial g(x,y) }{ \partial y })$ は接線の法線ベクトルを表す．</p>

<p>先程の</p>

<div class="mathjax">
$$
\begin{pmatrix} 
\frac{ \partial f(x,y) }{ \partial x } \\ 
\frac{ \partial f(x,y) }{ \partial y }
\end{pmatrix}
=
\lambda
\begin{pmatrix} 
\frac{ \partial g(x,y) }{ \partial x } \\ 
\frac{ \partial g(x,y) }{ \partial y }
\end{pmatrix}
$$
</div>

<p>より，ベクトル $\partial g$ とベクトル $\partial f$ が点 $(x_0,y_0)$ で並行であれば，その点で $f(x,y)$ が停留点を持つ．</p>

<p>「ベクトル $\partial g$ とベクトル $\partial f$ が点 $(x_0,y_0)$ で並行である」という部分が肝である．例えば，下図において，制約条件 $g(x,y)=0$ は赤の等高線， $f(x,y)$ はc1,c2,c3の等高線で表し，c1 &lt; c2 &lt; c3 とすると，勾配 $\nabla f$ は図のようになる(わざと $g(x,y)=0$ と $f(x,y)=c2$ が接するように描いている)．ラグランジュ乗数法により勾配 $\nabla g$ と勾配 $\nabla f$ が並行な点，すなわち(x0,y0)において， $g(x,y)=0$ を満たす $f(x,y)$ の停留点（下図の場合は最小値)となることがわかる．</p>

<p><img src="/img/lagrange_multiplier.svg" alt="lagrange_multiplier" /></p>

<h2 id="nabla-g-に対する1次独立性が必要">$\nabla g$ に対する1次独立性が必要</h2>

<p>制約条件 $g$ が複数ある場合，ラグランジュの未定乗数法によって最適解を求めるためには， $\nabla g$ がそれぞれ1次独立である必要がある．1次独立な $\nabla g$ と $\lambda$ の線形和によって， $\nabla f$ を表現する必要があることが理由．</p>

<h2 id="最適性の２次の十分条件を満たせば局所最適解">最適性の２次の十分条件を満たせば局所最適解</h2>

<p>関数 $f,g_1,&hellip;,g_p$ は二階微分可能とし，局所最適解を $x^*$ とする． $\nabla g_1(x^*),&hellip;,\nabla g_p(x^*)$ は１次独立であり，ラグランジュ乗数を $\lambda^*_1,&hellip;,\lambda^*_p \in \mathbb{ R }$ とする．</p>

<p>ここで，部分空間 $S$ を以下のように定義する．</p>

<div class="mathjax">
$$
S = \{y \in \mathbb{ R } | \nabla g_i(x^*)^T y =0, i=1,...,p \}
$$
</div>

<p>更に，行列 $L \in \mathbb{ R }^{n \times n}$ を以下のように定義する．</p>

<div class="mathjax">
$$
L = \nabla^2 f(x^*) + \displaystyle \sum_{ i = 1 }^{ m } \lambda_i^* \nabla^2 g_i(x^*)
$$
</div>

<p>これら $S,L$ について， $y \in S, y \neq 0$ を用いて，以下を満たせば $x^*$ は局所最適解である．</p>

<div class="mathjax">
$$
y^T L y \gt 0
$$
</div>

<p>つまり，ヘッセ行列$L$が正定値であれば， $x^*$ は局所最適解である．</p>

<p>ちなみに，２次の必要条件は，上記ヘッセ行列が半正定値であるという，十分条件を緩くしたものになる．</p>

<h2 id="ラグランジュ乗数法のkkt条件">ラグランジュ乗数法のKKT条件</h2>

<p>これまでは制約条件 $g(x,y)$ が0である元で議論してきたが， $g(x,y)$ に範囲がある場合を考える．</p>

<p>下図において， $g(x,y)=0$ を赤の等高線で表し，内側に向かって $g(x,y)$ が大きくなっているとすると， $g(x,y) \gt 0$ は楕円の内部に該当し，勾配 $\nabla g$ は図のような向きになる．また， $f(x,y)$ を緑の等高線で表し，c1 &lt; c2 &lt; c3 であるとすると，勾配 $\nabla f$ は図のような向きになる．</p>

<p><img src="/img/lagrange_multiplier_kkt1.svg" alt="lagrange_multiplier_kkt1" /></p>

<p>この場合，「制約条件 $g(x,y) \geq 0$ の元で $f(x,y)$ を最大にする点」をラグランジュ乗数法により，点(x0,y0)と求めることができる．</p>

<p>また，「制約条件 $g(x,y) \geq 0$ の元で $f(x,y)$ を最小にする点」は，ラグランジュ乗数法により，下図の点(x1,y1)と求めることができる．</p>

<p><img src="/img/lagrange_multiplier_kkt2.svg" alt="lagrange_multiplier_kkt2" /></p>

<p>更に，下図の場合，「制約条件 $g(x,y) \geq 0$ の元で $f(x,y)$ を最大にする点」は，c1 &lt; c2 &lt; c3 であるとするなら，ラグランジュ乗数法のλを0とし，単に $f(x,y)$ の最大点を求めれば良いので，点(x2,y2)と求まる．</p>

<p><img src="/img/lagrange_multiplier_kkt3.svg" alt="lagrange_multiplier_kkt3" /></p>

<p>これらの条件をまとめたものが以下のKKT条件(Karush-Kuhn-Tucker condition)である．</p>

<ul>
<li>$g(x,y) \geq 0$ の元で $f(x,y)$ を<strong>最大化</strong>するには以下を解く</li>
</ul>

<div class="mathjax">
$$
L(x,y,\lambda) = f(x,y) + \lambda g(x,y) \\
\frac{ \partial L }{ \partial x } = \frac{ \partial L }{ \partial y } = \frac{ \partial L }{ \partial \lambda } = 0 \\ 
g(x,y) \geq 0 \\
\lambda \geq 0 \\
\lambda g(x,y) = 0
$$
</div>

<ul>
<li>$g(x,y) \geq 0$ の元で $f(x,y)$ を<strong>最小化</strong>するには以下を解く</li>
</ul>

<div class="mathjax">
$$
L(x,y,\lambda) = f(x,y) - \lambda g(x,y) \\
\frac{ \partial L }{ \partial x } = \frac{ \partial L }{ \partial y } = \frac{ \partial L }{ \partial \lambda } = 0 \\
g(x,y) \geq 0 \\
\lambda \geq 0 \\
\lambda g(x,y) = 0
$$
</div>

<p>条件式がややこしく見えるが，言っていることは単純で，「最大化したいのか最小化したいのかに注意し，勾配 $\nabla f$ と $\nabla g$ の向きをλの正負で制御して，ラグランジュ乗数法を解け」ということである．</p>

<p>$\lambda g(x) = 0$ は相補性条件と呼ばれ， $\lambda$ または $g(x)$ のどちらかが0になることを表している．</p>

<h1 id="正則化とラグランジュ乗数法">正則化とラグランジュ乗数法</h1>

<p>以下の二乗和誤差と正則化項を合わせた関数 $E(\boldsymbol{w})$ を最小化したいとする．ξは正則化項の係数であり，正であるとする．</p>

<div class="mathjax">
$$
E(\boldsymbol{w})
=
\frac{1}{2} \sum_{n=1}^{N} \{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x_n} - t_n\}^2 + \xi \sum_{i=1}^{D} | w_i |^q
$$
</div>

<p>いま，以下の二乗和誤差の最小値を</p>

<div class="mathjax">
$$
f(\boldsymbol w) 
= 
\frac{1}{2} \sum_{n=1}^{N} \{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x_n} - t_n\}^2
$$
</div>

<p>以下の制約条件で求めるとする．</p>

<div class="mathjax">
$$
g(\boldsymbol w)
=
\eta - \sum_{i=1}^{D} | w_i |^q
$$
</div>

<p>$f(\boldsymbol w)$ の最小化には，KKT条件 $g(\boldsymbol w) \geq 0$ と $\lambda \geq 0$ の元で，以下のラグランジュ関数を解く必要がある．</p>

<div class="mathjax">
$$
L(\boldsymbol {w},\lambda) = f(\boldsymbol w) - \lambda g(\boldsymbol w) \\
\frac{ \partial L }{ \partial \boldsymbol{w} } = 0 
$$
</div>

<p>ここで，</p>

<div class="mathjax">
$$
\frac{ \partial L }{ \partial \boldsymbol{w} } = \frac{ \partial}{ \partial \boldsymbol{w} } \{ \frac{1}{2} \sum_{n=1}^{N} \{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x_n} - t_n\}^2 + \lambda \sum_{i=1}^{D} | w_i |^q \} = 0
$$
</div>

<p>であるので， $\lambda = \xi$ とすると，内部的には，二乗和誤差+正則化の最小値を求めていることになる． $\xi &gt; 0$ であるので， $\lambda &gt; 0$ である必要が出てくる．KKT条件 $\lambda g(\boldsymbol w) = 0$ より， $g(\boldsymbol w) = 0$ なので以下が成り立つ．</p>

<div class="mathjax">
$$
\eta = \sum_{i=1}^{D} | w_i |^q
$$
</div>

<p>q=1のときの $f(\boldsymbol w)$ と $g(\boldsymbol w)$ を以下のように図示してみた．</p>

<p><img src="/img/visualize_of_lasso.svg" alt="visualize_of_lasso" /></p>

<p>更に，断面を簡易的に図示してみた．ラグランジュ乗数法の仕組みにより，制約条件$g$の元で，関数$f$の停留点を求めるには，勾配ベクトル $\nabla g$ のスカラーλ倍したものが，勾配ベクトル $\nabla f$ と並行である必要があるので，停留点はもちろん，ηもλに依存する．</p>

<p><img src="/img/fragment_of_lasso.svg" alt="fragment_of_lasso" /></p>

<p>上図の停留点においてw1=0となっている．疎（スパース）な解が得られている例であり，lassoの特徴である．この特徴を用いることで，モデルの複雑さを抑えることができるが，必ずスパースな解が得られるとは限らない．例えば，下図のような場合はスパースでない解が得られ，関数f(w)が制約条件g(w)&gt;0内に入っている場合は，関数f(w)そのものの最小値となる．</p>

<p><img src="/img/visualize_of_lasso_without_sparse.svg" alt="visualize_of_lasso_without_sparse" /></p>

<h1 id="近接勾配法">近接勾配法</h1>

<p>たぶんつづく</p>

  </div>
  <footer class="post-footer">
    
    <div class="share">
    <a href="https://twitter.com/share?url=https%3a%2f%2fyasakura.me%2fpost%2fsubdifferential%2f&text=%e5%8a%a3%e5%be%ae%e5%88%86%e3%81%a8Lasso%e3%81%ab%e3%82%88%e3%82%8b%e3%82%b9%e3%83%91%e3%83%bc%e3%82%b9%e6%80%a7 - yasakura" rel="nofollow" target="_blank" class="tw">Twitter</a>
    <a href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fyasakura.me%2fpost%2fsubdifferential%2f" class="fb" target="_blank" rel="nofollow">Facebook</a>
    <a href="http://b.hatena.ne.jp/add?mode=confirm&url=https%3a%2f%2fyasakura.me%2fpost%2fsubdifferential%2f&title=%e5%8a%a3%e5%be%ae%e5%88%86%e3%81%a8Lasso%e3%81%ab%e3%82%88%e3%82%8b%e3%82%b9%e3%83%91%e3%83%bc%e3%82%b9%e6%80%a7 - yasakura" class="ht" target="_blank" rel="nofollow">Hatena</a>
</div>
  </footer>
</article>


<div class="post-widget">
  <div class="twitter-widget">
    <a class="twitter-timeline" data-lang="en" data-width="300" href="https://twitter.com/yasakurara?ref_src=twsrc%5Etfw">Tweets by yasakurara</a>
    <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
  </div>
</div>


<footer class="footer">
  <nav>
    <ul>
      <li><a href="https://yasakura.me//terms/">Terms of Use</a></li>
    </ul>
  </nav>
  <span>&copy; 2021 yasakura</span>
</footer>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    },
    linebreaks: {
      automatic: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
</body>
</html>

